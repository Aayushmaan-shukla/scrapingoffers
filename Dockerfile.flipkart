FROM python:3.10-slim

# Install system dependencies including Chrome
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    unzip \
    curl \
    xvfb \
    && rm -rf /var/lib/apt/lists/*

# Install Google Chrome
RUN apt-get update && apt-get install -y --no-install-recommends \
    wget gnupg ca-certificates unzip \
    libnss3 libxss1 libasound2 \
    libatk-bridge2.0-0 libgtk-3-0 libgbm1 libx11-xcb1 libxcomposite1 \
    libxcursor1 libxdamage1 libxi6 libxtst6 libxrandr2 libxrender1 \
    libdrm2 xdg-utils fonts-liberation && \
    rm -rf /var/lib/apt/lists/* && \
    wget -qO- https://dl.google.com/linux/linux_signing_key.pub | gpg --dearmor > /usr/share/keyrings/google.gpg && \
    echo "deb [signed-by=/usr/share/keyrings/google.gpg arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" > /etc/apt/sources.list.d/google-chrome.list && \
    apt-get update && apt-get install -y --no-install-recommends google-chrome-stable && \
    rm -rf /var/lib/apt/lists/*

# Set up Chrome environment variables
ENV CHROME_BIN=/usr/bin/google-chrome
ENV CHROME_PATH=/usr/bin/google-chrome


# Set working directory to match server location
WORKDIR /root/scraping/dockertest/scrapingoffers

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the enhanced Flipkart scraper
COPY enhanced_flipkart_scraper_comprehensive.py .

# Create data directory
RUN mkdir -p /root/scraping/dockertest/scrapingoffers/data

# Copy the all_data.json file to the data directory
COPY all_data.json ./data/

# Set environment variables for the scraper
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/root/scraping/dockertest/scrapingoffers

# Expose port for API mode (if needed)
EXPOSE 5001

# Default command - run the scraper
CMD ["python3", "enhanced_flipkart_scraper_comprehensive.py"]